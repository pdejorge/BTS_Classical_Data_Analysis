{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "A family of algorithms known as neural networks has become increasingly popular during the past few years under\n",
    "the name “deep learning.” While deep learning shows great promise in many machine\n",
    "learning applications, deep learning algorithms are often tailored very carefully to a\n",
    "specific use case. Here, we will only discuss some relatively simple methods, namely\n",
    "multilayer perceptrons for classification and regression, that can serve as a starting\n",
    "point for more involved deep learning methods. Multilayer perceptrons (MLPs) are\n",
    "also known as (vanilla) feed-forward neural networks, or sometimes just neural\n",
    "networks.\n",
    "\n",
    "MLPs can be viewed as generalizations of linear models that perform multiple stages\n",
    "of processing to come to a decision. Remember that the prediction by a linear regressor is given as:\n",
    "\n",
    "ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\n",
    "\n",
    "In plain English, ŷ is a weighted sum of the input features x[0] to x[p], weighted by\n",
    "the learned coefficients w[0] to w[p]. We could visualize this graphically as shown in the following figure:\n",
    "\n",
    "<img src=\"perceptron.png\">\n",
    "\n",
    "In an MLP this process of computing weighted sums is repeated multiple times, first\n",
    "computing hidden units that represent an intermediate processing step, which are\n",
    "again combined using weighted sums to yield the final result. Note that there can be several layers of hidden units, each of which will create a more complex representation of the input.\n",
    "\n",
    "<img src=\"MLP.png\">\n",
    "\n",
    "Computing a series of weighted sums is mathematically the same as computing just\n",
    "one weighted sum, so to make this model truly more powerful than a linear model,\n",
    "we need one extra trick. After computing a weighted sum for each hidden unit, a\n",
    "nonlinear function is applied to the result.\n",
    "\n",
    "Let’s look into the workings of the MLP by applying the `MLPClassifier` to the\n",
    "two_moons dataset. Import the `make_moons` method and generate a dataset with 100 samples `noise=0.25` and `random_state=3`. Then make a scatter plot of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the `Perceptron` classifier from sklearn and apply it to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a function to visualize the input data and the decision function of the model, you may go to the Random Forest exercise to look for inspiration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the classifier. Is this shape expected? Why is the perceptron a linear classifier? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import the `MLPClassifier`, set the random state to 0 and the solver to lbfgs. Leave the rest of the parameters as default. Fit the MLP model and then plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many hidden units does the model have by default? Is this a good idea for such a small data set? Try different numbers and see the effect on the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only 10 hidden units, the decision boundary looks somewhat more ragged. The\n",
    "default nonlinearity is relu, shown in Figure 2-46. With a single hidden layer, this\n",
    "means the decision function will be made up of 10 straight line segments. If we want\n",
    "a smoother decision boundary, we could add more hidden units,\n",
    "add a second hidden layer, or use the tanh nonlinearity. Try a model with 2 hidden layers of 10 units each, then try a model with one hidden layer of 10 units and tanh activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can also control the complexity of a neural network by using an l2 penalty\n",
    "to shrink the weights towards zero, as we did in ridge regression. The parameter for this in the MLPClassifier is alpha, and it is set to a very low value (little regularization) by default. Generate a plot with 4 different parameters for alpha [0.0001, 0.01, 1, 2] and for MLP with 2 layers of size 10 or 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important property of neural networks is that their weights are set randomly\n",
    "before learning is started, and this random initialization affects the model that is\n",
    "learned. That means that even when using exactly the same parameters, we can\n",
    "obtain very different models when using different random seeds. If the networks are\n",
    "large, and their complexity is chosen properly, this should not affect accuracy too\n",
    "much, but it is worth keeping in mind (particularly for smaller networks). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks and the Breast cancer dataset\n",
    "\n",
    "We will use the Breast cancer dataset to check an application of neural networks with real-world data. Load the breast cancer dataset from sklearn and print the description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the dataset target names, the feature names and the input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train perform a train test split with `random_state=0` and train a MLP with the default parameters and `random state=42`. After training print the accuracy on the training and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may compare the performance of the MLP with the SVC and the RandomForestClassifier. Remember to set `gamma='scale'` for the SVC and you can use `n_estimators=1000` for the Random Forest. Print the accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model has obtained the best performance? Why do you think that is? Compute the standard deviation for each feature in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now normalize the data, subtract the mean and divide by the standard deviation. You have to compute the mean and std on the training set, and use the same one for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing that, you can check that the mean and std has been actually set to 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run again the MLP on the normalized data and print the accuracy on the training and the test. Did the results improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have a warning saying that the optimization has not converged. This usually means we should add more iterations, set `max_iter` to 1000. What are the accuracies now? I there a way you can think of to further improve the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally compare the performance of the SVC and the random forest with the normalized data. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
