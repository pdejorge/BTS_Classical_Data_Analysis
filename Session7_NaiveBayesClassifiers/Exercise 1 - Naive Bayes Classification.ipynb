{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifiers\n",
    "\n",
    "Naive Bayes models are fast and simple classification algorithms, often suitable for very high-dimensional datasets. Because they are so fast and have few tunable parameters, they are a very useful baseline for a classification problem.\n",
    "\n",
    "## Bayes Theorem\n",
    "\n",
    "To understand how bayes classifiers work, first we need to explain the Bayes' theorem. Bayes' theorem is a mathematical expression describing the relationship of conditional probabilities of statistical variables. In Bayesian classification, we are interested in finding the probability of a label given some observed features, which we can write as  $P(L | features)$. Bayes' theorem helps us to derive this from other variables that we can compute more directly:\n",
    "\n",
    "$$ P(L~|~{\\rm features}) = \\frac{P({\\rm features}~|~L)P(L)}{P({\\rm features})}$$\n",
    "\n",
    "If we are dealing with a binary classification problem, that is we want to classify each example with one of two lables —we can call them L1 and L2— then one way to make this decision is to compute the ratio of the posterior probabilities for each label:\n",
    "\n",
    "$$\\frac{P(L_1~|~{\\rm features})}{P(L_2~|~{\\rm features})} = \\frac{P({\\rm features}~|~L_1)}{P({\\rm features}~|~L_2)}\\frac{P(L_1)}{P(L_2)}$$\n",
    "\n",
    "All we need now is some model by which we can compute $P(features | L_i)$ for each label. Such a model is called a *generative model* because it specifies the hypothetical random process that generates the data. Specifying this generative model for each label is the main piece of the training of such a Bayesian classifier. The general version of such a training step is a very difficult task, but we can make it simpler through the use of some simplifying assumptions about the form of this model.\n",
    "\n",
    "This is why the method is named \"naive Bayes\". If we make \"naive\" assumptions about the generative models that follows our data, we can find a rough approximation of the generative model for each class, and then proceed with the Bayesian classification. Different types of naive Bayes classifiers rest on different naive assumptions about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes\n",
    "In this classifier, the assumption is that data associated to each label is drawn from a *Gaussian distribution*. Let us start by generating some points with the function `make_blobs`. Generate 100 points with 2 centers, use `random_state=2` and `cluster_std=1.5`. Make a scatter plot of the generated points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume that points belonging to each class are generated from a gaussian distribution with no covariance between dimensions, (that is, the different features are independent between them) then we can easily approximate the generative distribution by computing the mean and the standard deviation of the points belonging to the two classes. The following function will plot the gaussian distributions fitted to the given points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gaussian_distributions(X, y):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n",
    "    ax.set_title('Naive Bayes Model', size=14)\n",
    "\n",
    "    xlim = 1.1*np.array([X[:, 0].min(), X[:, 0].max()]) \n",
    "    ylim = 1.1*np.array([X[:, 1].min(), X[:, 1].max()])\n",
    "\n",
    "    xg = np.linspace(xlim[0], xlim[1], 50)\n",
    "    yg = np.linspace(ylim[0], ylim[1], 50)\n",
    "    xx, yy = np.meshgrid(xg, yg)\n",
    "    Xgrid = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "    for label, color in enumerate(['red', 'blue']):\n",
    "        mask = (y == label)\n",
    "        mu, std = X[mask].mean(0), X[mask].std(0)\n",
    "        P = np.exp(-0.5 * (Xgrid - mu) ** 2 / std ** 2).prod(1)\n",
    "        ax.pcolorfast(xg, yg, P.reshape(xx.shape), alpha=0.5,\n",
    "                      cmap=color.title() + 's')\n",
    "        ax.contour(xx, yy, P.reshape(xx.shape),\n",
    "                   levels=[0.01, 0.1, 0.5, 0.9],\n",
    "                   colors=color, alpha=0.2)\n",
    "\n",
    "    ax.set(xlim=xlim, ylim=ylim);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the gaussian distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ellipses here represent the Gaussian generative model for each label, with larger probability toward the center of the ellipses. With this generative model in place for each class, we have a simple recipe to compute the likelihood $P(features | L_i)$ for any data point, and thus we can quickly compute the posterior ratio and determine which label is the most probable for a given point.\n",
    "\n",
    "This procedure is implemented in Scikit-Learn with `sklearn.naive_bayes.GaussianNB` estimator, import this model and fit it to the dataset we have generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate some new data and use the model to predict the label. Generate points uniformly between (-6, 8) for the x axis and (-14, 4) for the y axis. Set `np.random.seed(0)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now predict the label with the gaussian naive bayes's classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the decision boundary, plot the predictions with a scatter plot. Plot first the points of the training set and then plot the new generated points with `alpha=0.2`. What shape does the decision function follow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate a new model and change the priors, fit the model to the same training set and plot the prediction over the uniformly distributed data to see the decision function. How does the prior affect the decision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting property of this Bayesian formalism is that it naturally allows for probabilistic classification, which we can compute using the `predict_proba` method. Use it to obtain the posterior probabilities of the uniformly distributed data. Use the model without predefined priors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now represent the original training set and a heatmap of the posterior probabilities. You have to define a new function `plot_posterior_proba` that receives `X, y, model` and plots the points in `X` and a heatmap of the posterior probability. To do so you may reuse part of the code provided above to plot the gaussian distributions. Use the `cmap='jet'` with the `pcolorfast` method. And plot the posterior probability of class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the posterior probability. Is this shape expected? What observations do you have about the \"border\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes's classifier are very fast algorithms, and as we have seen, very robust to variations in the prior distributions. But what happens if the data does not follow exactly a gaussian distribution? Now generate a new dataset with `make_circles`. Generate 100 points with `factor=.1, noise=.2, random_state=0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a GaussianNB and plot the posterior probability. What do you observe? Was this expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the gaussian distributions fitted to this data. What shape do they have? Is the \"naive\" assumption correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate a dataset which does not have \"circular\" classes. Use the `make_moons` method from sklearn with `noise=0.2`. Now fit another model an plot the posterior probability and the gaussian distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial  Bayes' classifier\n",
    "The Gaussian assumption just described is not the only assumption that could be used about the generative distribution for each label. Another useful example is *multinomial naive Bayes*, where the features are assumed to be generated from a *multinomial distribution*. The multinomial distribution describes the probability of observing counts among a number of categories, and thus multinomial naive Bayes is most appropriate for features that represent counts or count rates.\n",
    "\n",
    "The idea is precisely the same as before, except that instead of modeling the data distribution with the best-fit Gaussian, we model the data distribuiton with a best-fit multinomial distribution.\n",
    "\n",
    "### Example: Classifying text\n",
    "One place where multinomial naive Bayes might be useful is in text classification, where the features are related to word counts or frequencies within the documents to be classified. Here we will use the sparse word count features from the 20 Newsgroups corpus to show how we might classify these short documents into categories. Let's download the data and take a look at the target names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the dataset description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the target names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity here, we will select just a few of these categories, and download the training and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the data from example number 6 in the training set (remember python starts counting from 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use this data for machine learning, we need to be able to convert the content of each string into a vector of numbers. For this we will use the TF-IDF vectorizer. The Term frequency-inverse document frequency (TF–IDF) which weights the word counts by a measure of how often they appear in the documents. We will compute the tf-idf and create a pipeline that attaches it to a multinomial naive Bayes classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a pipeline with the `TfidfVectorizer`and `MultinomialNB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this pipeline, now apply it to the training data, and predict labels for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have predicted the labels for the test data, we can evaluate them to learn about the performance of the estimator. Plot the confusion matrix of the predictions. Use the `confusion_matrix` from sklearn and the `sns.heatmap`. What can you say about this confusion matrix? Are all the classes equally well separated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting experiment that we can do now is to try to determine the category for any string, using this pipeline. This is a quick function that will return the prediction for a single string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(s, train=train, model=model):\n",
    "    pred = model.predict([s])\n",
    "    return train.target_names[pred[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try different sentences and see how are they classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
